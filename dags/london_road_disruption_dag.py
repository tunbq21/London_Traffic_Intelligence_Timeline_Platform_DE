from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.sdk import TaskGroup
from hooks.tfl_hook import TfLHook
from datetime import datetime, timedelta
import json, os
import logging
import pandas as pd
import csv
from pathlib import Path
import psycopg2

# Danh sách cột cố định
COLUMNS = [
    "id", "category", "severity", "location", "comments", "startDateTime", 
    "endDateTime", "lastModDateTime", "point"
]

def extract_road_disruption_data():
    try: 
        hook = TfLHook()
        data = hook.get_data("Road/All/Disruption")
        if not data:
            logging.info("Không có dữ liệu từ API.")
            return None

        folder_path = "/usr/local/airflow/include/json/road_disruption"
        Path(folder_path).mkdir(parents=True, exist_ok=True)
        file_path = f"{folder_path}/road_disruption_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        
        chunk_size = 200
        for i in range(0, len(data), chunk_size):
            chunk = data[i : i + chunk_size]
            df = pd.json_normalize(chunk)
            for col in COLUMNS:
                if col not in df.columns:
                    df[col] = None
            
            df_filtered = df[COLUMNS].copy()
            # Xử lý tọa độ point thành chuỗi JSON
            df_filtered['point'] = df_filtered['point'].apply(
                lambda x: json.dumps(x) if isinstance(x, (list, dict)) else x
            )
            df_filtered.to_json(file_path, orient='records', lines=True, mode='a')
            
        return file_path
    except Exception as e:
        logging.error(f"Lỗi extract: {e}")
        raise

def jsonl_to_csv(**kwargs):
    try:
        ti = kwargs['ti']
        jsonl_file_path = ti.xcom_pull(task_ids='extraction_group.extract_road_disruption_data')
        if not jsonl_file_path: return None
            
        df = pd.read_json(jsonl_file_path, lines=True)
        df = df[COLUMNS]
        
        csv_file_path = jsonl_file_path.replace('.jsonl', '.csv').replace('/json/', '/csv/')
        Path(csv_file_path).parent.mkdir(parents=True, exist_ok=True)
        
        # Lưu CSV với các ô trống thực sự để Postgres dễ nhận diện NULL
        df.to_csv(csv_file_path, index=False, quoting=csv.QUOTE_MINIMAL, na_rep='')
        return csv_file_path
    except Exception as e:
        logging.error(f"Lỗi convert CSV: {e}")
        raise

def create_road_table():
    pg_hook = PostgresHook(postgres_conn_id='local_postgres')
    # DROP bảng cũ để đồng bộ kiểu dữ liệu mới
    pg_hook.run("DROP TABLE IF EXISTS london_road_disruptions;")
    
    create_sql = """
    CREATE TABLE london_road_disruptions (
        id VARCHAR(100) PRIMARY KEY,
        category VARCHAR(100),
        severity VARCHAR(100),
        location TEXT,
        comments TEXT,
        startDateTime TEXT, -- Chuyển sang TEXT để tránh lỗi định dạng khi load
        endDateTime TEXT,
        lastModDateTime TEXT,
        point TEXT,
        extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
    pg_hook.run(create_sql)

def load_csv_to_postgres(**kwargs):
    ti = kwargs['ti']
    csv_file_path = ti.xcom_pull(task_ids='extraction_group.jsonl_to_csv')
    if not csv_file_path: return

    pg_hook = PostgresHook(postgres_conn_id='local_postgres')
    
    copy_sql = """
        COPY london_road_disruptions(id, category, severity, location, comments, startDateTime, endDateTime, lastModDateTime, point)
        FROM STDIN WITH (FORMAT CSV, HEADER, DELIMITER ',', NULL '');
    """
    pg_hook.copy_expert(sql=copy_sql, filename=csv_file_path)




def create_road_table_azure():
    try:
        conn_uri = os.getenv('AIRFLOW_CONN_AZURE_POSTGRES_CONN')
        conn = psycopg2.connect(conn_uri)
        
        cur = conn.cursor()
        create_sql = """
    CREATE TABLE IF NOT EXISTS london_road_disruptions (
        id VARCHAR(100) PRIMARY KEY,
        category VARCHAR(100),
        severity VARCHAR(100),
        location TEXT,
        comments TEXT,
        startDateTime TEXT,
        endDateTime TEXT,
        lastModDateTime TEXT,
        point TEXT,
        extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
    """
        cur.execute(create_sql)
        conn.commit()
        cur.close()
        conn.close()
        logging.info("Kết nối và tạo bảng Azure thành công bằng psycopg2 + ENV!")
    except Exception as e:
        logging.error(f"Lỗi: {e}")
        raise


def load_csv_to_azure_postgres(**kwargs):
    ti = kwargs['ti']
    csv_file_path = ti.xcom_pull(task_ids='extraction_group.jsonl_to_csv')
    
    if not csv_file_path:
        logging.warning("Không tìm thấy đường dẫn file CSV.")
        return

    pg_hook = PostgresHook(postgres_conn_id='azure_postgres_conn')
    
    # Lấy connection object thực sự để giữ Session không bị đóng
    conn = pg_hook.get_conn()
    cur = conn.cursor()
    
    try:
        # 1. Tạo bảng tạm (Trên cùng 1 cursor/session)
        cur.execute("CREATE TEMP TABLE temp_london_road (LIKE london_road_disruptions INCLUDING ALL);")
        
        # 2. Sử dụng copy_expert với cursor hiện tại
        copy_query = """
            COPY temp_london_road(id, category, severity, location, comments, 
                                 startDateTime, endDateTime, lastModDateTime, point) 
            FROM STDIN WITH (FORMAT CSV, HEADER, DELIMITER ',', NULL '');
        """
        with open(csv_file_path, 'r', encoding='utf-8') as f:
            cur.copy_expert(sql=copy_query, file=f)
            
        # 3. Thực hiện Upsert từ bảng tạm vào bảng chính
        upsert_sql = """
            INSERT INTO london_road_disruptions (id, category, severity, location, comments, 
                                                startDateTime, endDateTime, lastModDateTime, point)
            SELECT id, category, severity, location, comments, 
                   startDateTime, endDateTime, lastModDateTime, point 
            FROM temp_london_road
            ON CONFLICT (id) DO UPDATE SET 
                category = EXCLUDED.category,
                severity = EXCLUDED.severity,
                location = EXCLUDED.location,
                comments = EXCLUDED.comments,
                startDateTime = EXCLUDED.startDateTime,
                endDateTime = EXCLUDED.endDateTime,
                lastModDateTime = EXCLUDED.lastModDateTime,
                point = EXCLUDED.point,
                extracted_at = CURRENT_TIMESTAMP;
        """
        cur.execute(upsert_sql)
        
        # Quan trọng: Commit để lưu thay đổi và đóng bảng tạm
        conn.commit()
        logging.info("Upsert thành công lên Azure Postgres!")
        
    except Exception as e:
        conn.rollback() # Hoàn tác nếu lỗi
        logging.error(f"Lỗi khi xử lý database: {e}")
        raise
    finally:
        cur.close()
        conn.close()

default_args = {
    'owner': 'Tuan Quang',
    'start_date': datetime(2026, 1, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    dag_id='london_road_disruption_dag',
    default_args=default_args,
    schedule=None,
    catchup=False
) as dag:

    create_table_task = PythonOperator(
        task_id='create_road_table',
        python_callable=create_road_table
    )

    with TaskGroup('extraction_group') as extraction_group:
        extract_task = PythonOperator(
            task_id='extract_road_disruption_data',
            python_callable=extract_road_disruption_data
        )
        jsonl_to_csv_task = PythonOperator(
            task_id='jsonl_to_csv',
            python_callable=jsonl_to_csv,
        )
        extract_task >> jsonl_to_csv_task

    load_data_task = PythonOperator(
        task_id='load_csv_to_postgres',
        python_callable=load_csv_to_postgres,
    )

    create_table_filess_task = PythonOperator(
        task_id='create_road_table_azure',
        python_callable=create_road_table_azure,
    )

    load_data_filess_task = PythonOperator(
        task_id='load_csv_to_azure_postgres',
        python_callable=load_csv_to_azure_postgres,
    )


    [create_table_task, create_table_filess_task] >> extraction_group 

    extraction_group >> [load_data_task, load_data_filess_task]